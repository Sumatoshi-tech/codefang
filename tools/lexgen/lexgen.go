// lexgen converts Chen-Skiena sentiment lexicon text files into an embedded Go source file.
//
// Usage:
//
//	go run tools/lexgen/lexgen.go \
//	  -pos /tmp/pos_clean.txt \
//	  -neg /tmp/neg_clean.txt \
//	  -o pkg/analyzers/sentiment/lexicons/lexicon_data.gen.go
//
// The input files use the format: "word lang_code\n" (one per line).
// Positive words receive valence +1.5, negative words receive -1.5.
package main

import (
	"bufio"
	"flag"
	"fmt"
	"log"
	"os"
	"sort"
	"strings"
	"time"
	"unicode/utf8"
)

const (
	positiveValence = 1.5
	negativeValence = -1.5
	minWordLength   = 2
)

// targetLanguages are the languages we embed. Keeps binary size reasonable.
var targetLanguages = map[string]string{
	"ru":  "Russian",
	"zh":  "Chinese",
	"ja":  "Japanese",
	"ko":  "Korean",
	"es":  "Spanish",
	"fr":  "French",
	"de":  "German",
	"pt":  "Portuguese",
	"it":  "Italian",
	"nl":  "Dutch",
	"pl":  "Polish",
	"sv":  "Swedish",
	"cs":  "Czech",
	"tr":  "Turkish",
	"ar":  "Arabic",
	"hi":  "Hindi",
	"th":  "Thai",
	"vi":  "Vietnamese",
	"uk":  "Ukrainian",
	"fi":  "Finnish",
	"da":  "Danish",
	"no":  "Norwegian",
	"el":  "Greek",
	"hu":  "Hungarian",
	"ro":  "Romanian",
	"bg":  "Bulgarian",
	"hr":  "Croatian",
	"sk":  "Slovak",
	"he":  "Hebrew",
	"id":  "Indonesian",
	"ms":  "Malay",
	"fa":  "Persian",
}

type lexEntry struct {
	word    string
	valence float64
}

func main() {
	posFile := flag.String("pos", "", "Path to positive words file")
	negFile := flag.String("neg", "", "Path to negative words file")
	outFile := flag.String("o", "", "Output Go file path")
	flag.Parse()

	if *posFile == "" || *negFile == "" || *outFile == "" {
		flag.Usage()
		os.Exit(1)
	}

	entries := make(map[string][]lexEntry)

	loadFile(*posFile, positiveValence, entries)
	loadFile(*negFile, negativeValence, entries)

	generateGo(entries, *outFile)
}

func loadFile(path string, valence float64, entries map[string][]lexEntry) {
	f, err := os.Open(path)
	if err != nil {
		log.Fatalf("open %s: %v", path, err)
	}
	defer f.Close()

	scanner := bufio.NewScanner(f)
	lineNo := 0
	loaded := 0

	for scanner.Scan() {
		lineNo++
		line := strings.TrimSpace(scanner.Text())

		if line == "" {
			continue
		}

		lastSpace := strings.LastIndex(line, " ")
		if lastSpace < 0 {
			continue
		}

		word := strings.TrimSpace(line[:lastSpace])
		lang := strings.TrimSpace(line[lastSpace+1:])

		if _, ok := targetLanguages[lang]; !ok {
			continue
		}

		if utf8.RuneCountInString(word) < minWordLength {
			continue
		}

		entries[lang] = append(entries[lang], lexEntry{word: word, valence: valence})
		loaded++
	}

	if err := scanner.Err(); err != nil {
		log.Fatalf("scan %s: %v", path, err)
	}

	log.Printf("loaded %d entries from %s", loaded, path)
}

func generateGo(entries map[string][]lexEntry, outPath string) {
	f, err := os.Create(outPath)
	if err != nil {
		log.Fatalf("create %s: %v", outPath, err)
	}
	defer f.Close()

	w := bufio.NewWriter(f)

	fmt.Fprintf(w, "// Code generated by tools/lexgen/lexgen.go at %s. DO NOT EDIT.\n",
		time.Now().UTC().Format("2006-01-02"))
	fmt.Fprintln(w, "// Source: Chen & Skiena (2014) \"Building Sentiment Lexicons for All Major Languages\"")
	fmt.Fprintln(w, "// ACL 2014, https://aclanthology.org/P14-2063/")
	fmt.Fprintln(w, "")
	fmt.Fprintln(w, "package lexicons")
	fmt.Fprintln(w, "")

	sortedLangs := make([]string, 0, len(entries))
	for lang := range entries {
		sortedLangs = append(sortedLangs, lang)
	}

	sort.Strings(sortedLangs)

	totalEntries := 0

	for _, lang := range sortedLangs {
		words := entries[lang]
		name := targetLanguages[lang]

		deduped := dedupe(words)

		sort.Slice(deduped, func(i, j int) bool {
			return deduped[i].word < deduped[j].word
		})

		funcName := strings.ToLower(name) + "Lexicon"

		fmt.Fprintf(w, "//nolint:gosmopolitan // generated multilingual lexicon data\n")
		fmt.Fprintf(w, "func %s() []Entry {\n", funcName)
		fmt.Fprintf(w, "\treturn []Entry{\n")

		for _, entry := range deduped {
			fmt.Fprintf(w, "\t\t{%q, %.1f},\n", entry.word, entry.valence)
		}

		fmt.Fprintln(w, "\t}")
		fmt.Fprintln(w, "}")
		fmt.Fprintln(w, "")

		totalEntries += len(deduped)
		log.Printf("%s (%s): %d entries", name, lang, len(deduped))
	}

	if err := w.Flush(); err != nil {
		log.Fatalf("flush: %v", err)
	}

	log.Printf("generated %s: %d languages, %d total entries", outPath, len(sortedLangs), totalEntries)
}

func dedupe(entries []lexEntry) []lexEntry {
	seen := make(map[string]float64)

	for _, e := range entries {
		if existing, ok := seen[e.word]; ok {
			if existing > 0 && e.valence < 0 {
				seen[e.word] = 0
			}
		} else {
			seen[e.word] = e.valence
		}
	}

	result := make([]lexEntry, 0, len(seen))
	for word, valence := range seen {
		if valence == 0 {
			continue
		}

		result = append(result, lexEntry{word: word, valence: valence})
	}

	return result
}
